{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2278629d",
   "metadata": {},
   "source": [
    "### Step 1: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b9e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring the dataset to understand its structure\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv('questions.csv')  \n",
    "\n",
    "# Checking the dataset structure\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Checking for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Checking the distribution of duplicate vs non-duplicate questions\n",
    "if 'is_duplicate' in df.columns:\n",
    "    print(\"\\nDuplicate Distribution:\")\n",
    "    print(df['is_duplicate'].value_counts())\n",
    "    print(\"Percentage of duplicates:\", (df['is_duplicate'].sum() / len(df)) * 100, \"%\")\n",
    "\n",
    "# displaying some example question pairs\n",
    "print(\"\\n--- Example Question Pairs ---\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"Question 1: {df.iloc[i]['question1']}\")\n",
    "    print(f\"Question 2: {df.iloc[i]['question2']}\")\n",
    "    if 'is_duplicate' in df.columns:\n",
    "        print(f\"Is Duplicate: {df.iloc[i]['is_duplicate']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4bdeb",
   "metadata": {},
   "source": [
    "### Step 2: Data Preparation and Cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ca218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv('questions.csv')  \n",
    "\n",
    "print(\"=== FAST DATA PREPARATION ===\")\n",
    "\n",
    "# Step 2.1: Basic Cleaning\n",
    "print(\"1. Cleaning the data...\")\n",
    "df_clean = df.dropna(subset=['question1', 'question2'])\n",
    "print(f\"Removed {len(df) - len(df_clean)} rows with missing questions\")\n",
    "\n",
    "# Converting to string\n",
    "df_clean['question1'] = df_clean['question1'].astype(str)\n",
    "df_clean['question2'] = df_clean['question2'].astype(str)\n",
    "\n",
    "# Step 2.2: Extracting unique questions using pandas\n",
    "print(\"\\n2. Extracting unique questions (FAST method)...\")\n",
    "\n",
    "# Combining both question columns into one series\n",
    "all_questions_series = pd.concat([\n",
    "    df_clean['question1'], \n",
    "    df_clean['question2']\n",
    "], ignore_index=True)\n",
    "\n",
    "# Getting unique questions \n",
    "unique_questions = all_questions_series.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(f\"Total unique questions found: {len(unique_questions)}\")\n",
    "\n",
    "# Step 2.3: Createing questions database\n",
    "print(\"\\n3. Creating questions database...\")\n",
    "questions_db = pd.DataFrame({\n",
    "    'question_id': range(len(unique_questions)),\n",
    "    'question_text': unique_questions\n",
    "})\n",
    "\n",
    "# Adding metadata\n",
    "questions_db['question_length'] = questions_db['question_text'].str.len()\n",
    "questions_db['word_count'] = questions_db['question_text'].str.split().str.len()\n",
    "\n",
    "print(f\"Database shape: {questions_db.shape}\")\n",
    "print(\"\\nSample questions:\")\n",
    "print(questions_db.head(10))\n",
    "\n",
    "# Step 2.4: Saving files\n",
    "print(\"\\n4. Saving files...\")\n",
    "questions_db.to_csv('unique_questions.csv', index=False)\n",
    "print(\"Saved unique questions to 'unique_questions.csv'\")\n",
    "\n",
    "# Creating a smaller sample for testing\n",
    "sample_size = 1000\n",
    "questions_sample = questions_db.head(sample_size).copy()\n",
    "questions_sample.to_csv('questions_sample.csv', index=False)\n",
    "print(f\"Created sample dataset with {sample_size} questions\")\n",
    "\n",
    "# Step 2.5: Statistics\n",
    "print(f\"\\n=== DATASET STATISTICS ===\")\n",
    "print(f\"Original dataset: {len(df):,} question pairs\")\n",
    "print(f\"After cleaning: {len(df_clean):,} question pairs\") \n",
    "print(f\"Unique questions: {len(unique_questions):,}\")\n",
    "print(f\"Average question length: {questions_db['question_length'].mean():.1f} characters\")\n",
    "print(f\"Average word count: {questions_db['word_count'].mean():.1f} words\")\n",
    "print(f\"Shortest question: {questions_db['question_length'].min()} characters\")\n",
    "print(f\"Longest question: {questions_db['question_length'].max()} characters\")\n",
    "\n",
    "# Showing examples\n",
    "print(f\"\\n=== EXAMPLE QUESTIONS ===\")\n",
    "try:\n",
    "    short_q = questions_db[questions_db['question_length'] < 30].iloc[0]\n",
    "    print(f\"Short: {short_q['question_text']}\")\n",
    "except:\n",
    "    print(\"Short: No questions under 30 characters\")\n",
    "\n",
    "try:\n",
    "    medium_q = questions_db[(questions_db['question_length'] >= 50) & \n",
    "                           (questions_db['question_length'] <= 100)].iloc[0]\n",
    "    print(f\"Medium: {medium_q['question_text']}\")\n",
    "except:\n",
    "    print(\"Medium: No questions in 50-100 character range\")\n",
    "\n",
    "try:\n",
    "    long_q = questions_db[questions_db['question_length'] > 150].iloc[0]\n",
    "    print(f\"Long: {long_q['question_text']}\")\n",
    "except:\n",
    "    print(\"Long: No questions over 150 characters\")\n",
    "\n",
    "print(\"Data cleaned and prepared\")\n",
    "print(\"Unique questions database created\") \n",
    "print(\"Files saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10d574",
   "metadata": {},
   "source": [
    "### Step 3: SBERT Embeddings Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"=== SBERT EMBEDDINGS GENERATION ===\")\n",
    "\n",
    "# Step 4.1: Load the pre-trained SBERT model\n",
    "print(\"1. Loading SBERT model...\")\n",
    "# Using a model optimized for semantic similarity\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast and good quality\n",
    "# Alternative models you can try:\n",
    "# model = SentenceTransformer('all-mpnet-base-v2')  # Higher quality, slower\n",
    "# model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # Multilingual\n",
    "\n",
    "print(f\"Model loaded: {model}\")\n",
    "\n",
    "# Step 4.2: Load your prepared questions\n",
    "print(\"\\n2. Loading questions...\")\n",
    "questions_df = pd.read_csv('unique_questions.csv')\n",
    "print(f\"Loaded {len(questions_df):,} unique questions\")\n",
    "\n",
    "# For testing, let's start with a smaller batch\n",
    "BATCH_SIZE = 10000  # Process 10k questions first for testing\n",
    "questions_sample = questions_df.head(BATCH_SIZE).copy()\n",
    "\n",
    "questions_list = questions_sample['question_text'].tolist()\n",
    "print(f\"Processing {len(questions_list):,} questions\")\n",
    "\n",
    "# Step 4.3: Generate embeddings with progress tracking\n",
    "print(\"\\n3. Generating SBERT embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate embeddings in batches to avoid memory issues\n",
    "batch_size = 500  # Process 500 questions at a time\n",
    "embeddings_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(questions_list), batch_size), desc=\"Generating embeddings\"):\n",
    "    batch = questions_list[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch, \n",
    "                                   convert_to_tensor=False,\n",
    "                                   show_progress_bar=False)\n",
    "    embeddings_list.extend(batch_embeddings)\n",
    "\n",
    "# Convert to numpy array\n",
    "embeddings = np.array(embeddings_list)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Generated embeddings in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Each question represented as {embeddings.shape[1]}-dimensional vector\")\n",
    "\n",
    "# Step 4.4: Save embeddings and metadata\n",
    "print(\"\\n4. Saving embeddings and metadata...\")\n",
    "\n",
    "# Save embeddings\n",
    "np.save('question_embeddings.npy', embeddings)\n",
    "print(\"Saved embeddings to 'question_embeddings.npy'\")\n",
    "\n",
    "# Save metadata\n",
    "embeddings_metadata = {\n",
    "    'model_name': 'all-MiniLM-L6-v2',\n",
    "    'embedding_dim': embeddings.shape[1],\n",
    "    'num_questions': embeddings.shape[0],\n",
    "    'generation_time': end_time - start_time,\n",
    "    'batch_size': BATCH_SIZE\n",
    "}\n",
    "\n",
    "with open('embeddings_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings_metadata, f)\n",
    "print(\"Saved metadata to 'embeddings_metadata.pkl'\")\n",
    "\n",
    "# Save the questions with IDs for reference\n",
    "questions_sample.to_csv('processed_questions.csv', index=False)\n",
    "print(\"Saved processed questions to 'processed_questions.csv'\")\n",
    "\n",
    "# Step 4.5: Quick quality check\n",
    "print(\"\\n5. Quality check...\")\n",
    "print(f\"Sample embedding (first 10 dimensions): {embeddings[0][:10]}\")\n",
    "print(f\"Embedding statistics:\")\n",
    "print(f\"  - Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"  - Std: {embeddings.std():.4f}\")\n",
    "print(f\"  - Min: {embeddings.min():.4f}\")\n",
    "print(f\"  - Max: {embeddings.max():.4f}\")\n",
    "\n",
    "print(\"\\n=== EMBEDDINGS GENERATION COMPLETE ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d491688",
   "metadata": {},
   "source": [
    "### Step 4:  Implement Cosine Similarity Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546881cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "class QuestionSimilarityFinder:\n",
    "    def __init__(self):\n",
    "        self.embeddings = None\n",
    "        self.questions_df = None\n",
    "        self.model = None\n",
    "        self.metadata = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Load all necessary data and models\"\"\"\n",
    "        print(\"=== LOADING SIMILARITY SEARCH SYSTEM ===\")\n",
    "        \n",
    "        # Load embeddings\n",
    "        print(\"1. Loading embeddings...\")\n",
    "        self.embeddings = np.load('question_embeddings.npy')\n",
    "        print(f\"✓ Loaded embeddings: {self.embeddings.shape}\")\n",
    "        \n",
    "        # Load questions\n",
    "        print(\"2. Loading questions...\")\n",
    "        self.questions_df = pd.read_csv('processed_questions.csv')\n",
    "        print(f\"✓ Loaded {len(self.questions_df):,} questions\")\n",
    "        \n",
    "        # Load model for encoding new questions\n",
    "        print(\"3. Loading SBERT model...\")\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        print(\"✓ Model loaded\")\n",
    "        \n",
    "        # Load metadata\n",
    "        with open('embeddings_metadata.pkl', 'rb') as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        print(\"✓ Metadata loaded\")\n",
    "        \n",
    "        print(\"✓ System ready for similarity search!\")\n",
    "        \n",
    "    def find_similar_questions(self, query_question, top_k=5, similarity_threshold=0.5):\n",
    "        \"\"\"\n",
    "        Find the most similar questions to a given query\n",
    "        \n",
    "        Args:\n",
    "            query_question (str): The question to find similarities for\n",
    "            top_k (int): Number of top similar questions to return\n",
    "            similarity_threshold (float): Minimum similarity score to consider\n",
    "            \n",
    "        Returns:\n",
    "            list: List of similar questions with their similarity scores\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== FINDING SIMILAR QUESTIONS ===\")\n",
    "        print(f\"Query: '{query_question}'\")\n",
    "        \n",
    "        # Encode the query question\n",
    "        start_time = time.time()\n",
    "        query_embedding = self.model.encode([query_question])\n",
    "        \n",
    "        # Calculate cosine similarity with all questions\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k similar questions\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        end_time = time.time()\n",
    "        \n",
    "        results = []\n",
    "        for i, idx in enumerate(top_indices):\n",
    "            similarity_score = similarities[idx]\n",
    "            if similarity_score >= similarity_threshold:\n",
    "                question_data = self.questions_df.iloc[idx]\n",
    "                results.append({\n",
    "                    'rank': i + 1,\n",
    "                    'question_id': question_data['question_id'],\n",
    "                    'question_text': question_data['question_text'],\n",
    "                    'similarity_score': similarity_score,\n",
    "                    'question_length': question_data['question_length'],\n",
    "                    'word_count': question_data['word_count']\n",
    "                })\n",
    "        \n",
    "        print(f\"✓ Search completed in {end_time - start_time:.4f} seconds\")\n",
    "        print(f\"✓ Found {len(results)} similar questions above threshold ({similarity_threshold})\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_results(self, results):\n",
    "        \"\"\"Display search results in a formatted way\"\"\"\n",
    "        if not results:\n",
    "            print(\"No similar questions found above the threshold.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"SIMILAR QUESTIONS FOUND:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for result in results:\n",
    "            print(f\"\\nRank {result['rank']} | Similarity: {result['similarity_score']:.4f}\")\n",
    "            print(f\"Question: {result['question_text']}\")\n",
    "            #print(f\"Stats: {result['word_count']} words, {result['question_length']} characters\")\n",
    "            print(\"-\" * 60)\n",
    "    \n",
    "\n",
    "# Usage example\n",
    "def main():\n",
    "    # Initialize the similarity finder\n",
    "    finder = QuestionSimilarityFinder()\n",
    "    finder.load_data()\n",
    "    \n",
    "    # Example searches\n",
    "    test_questions = [\n",
    "        \"How can I make money online?\",\n",
    "        \"What is machine learning?\",\n",
    "        \"How to learn Python programming?\",\n",
    "        \"What are the best ways to invest money?\",\n",
    "        \"How can I lose weight fast?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TESTING SIMILARITY SEARCH\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for query in test_questions:\n",
    "        results = finder.find_similar_questions(\n",
    "            query_question=query,\n",
    "            top_k=5,\n",
    "            similarity_threshold=0.8  \n",
    "        )\n",
    "        finder.display_results(results)\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "    \n",
    "    return finder\n",
    "\n",
    "# Interactive search function\n",
    "def interactive_search():\n",
    "    \"\"\"Interactive search where user can input questions\"\"\"\n",
    "    finder = QuestionSimilarityFinder()\n",
    "    finder.load_data()\n",
    "    \n",
    "    print(\"\\nINTERACTIVE QUESTION SIMILARITY SEARCH\")\n",
    "    print(\"Type your questions to find similar ones!\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Enter your question: \").strip()\n",
    "        \n",
    "        if query.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        if not query:\n",
    "            print(\"Please enter a valid question.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            results = finder.find_similar_questions(\n",
    "                query_question=query,\n",
    "                top_k=5,\n",
    "                similarity_threshold=0.7\n",
    "            )\n",
    "            finder.display_results(results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the main example\n",
    "    finder = main()\n",
    "    \n",
    "    #interactive_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49158fc8",
   "metadata": {},
   "source": [
    "### Batch Similarity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46548980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the finder\n",
    "finder = QuestionSimilarityFinder()\n",
    "finder.load_data()\n",
    "\n",
    "\n",
    "def batch_similarity_check(self, question_pairs):\n",
    "        \"\"\"\n",
    "        Check similarity for multiple question pairs\n",
    "        Useful for evaluation on the original Quora dataset\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== BATCH SIMILARITY CHECK ===\")\n",
    "        results = []\n",
    "        \n",
    "        for i, (q1, q2) in enumerate(question_pairs):\n",
    "            # Encode both questions\n",
    "            embeddings = self.model.encode([q1, q2])\n",
    "            \n",
    "            # Calculate similarity\n",
    "            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "            \n",
    "            results.append({\n",
    "                'pair_id': i,\n",
    "                'question1': q1,\n",
    "                'question2': q2,\n",
    "                'similarity_score': similarity\n",
    "            })\n",
    "            \n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processed {i} pairs...\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "df = pd.read_csv('questions.csv')\n",
    "dataset_pairs = list(zip(df['question1'].head(100), df['question2'].head(100)))\n",
    "results = finder.batch_similarity_check(dataset_pairs)\n",
    "\n",
    "# Display results in a readable format\n",
    "print(\"\\nRESULTS:\")\n",
    "print(\"=\" * 80)\n",
    "for r in results:\n",
    "    print(f\"\\nPair {r['pair_id']}:\")\n",
    "    print(f\"Question 1: {r['question1']}\")\n",
    "    print(f\"Question 2: {r['question2']}\")\n",
    "    print(f\"Similarity Score: {r['similarity_score']:.4f}\")\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd9d03b",
   "metadata": {},
   "source": [
    "### Step 5: Evaluation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "class QuestionSimilarityEvaluator:\n",
    "    def __init__(self):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.results = {}\n",
    "        \n",
    "    def load_test_data(self, sample_size=1000):\n",
    "        \"\"\"Load and prepare test data from original Quora dataset\"\"\"\n",
    "        print(\"=== LOADING EVALUATION DATA ===\")\n",
    "        \n",
    "        # Load the original dataset with labels\n",
    "        df = pd.read_csv('questions.csv')\n",
    "        df_clean = df.dropna(subset=['question1', 'question2', 'is_duplicate'])\n",
    "        \n",
    "        # Take a random sample for evaluation\n",
    "        test_data = df_clean.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        print(f\"✓ Loaded {len(test_data)} question pairs for evaluation\")\n",
    "        print(f\"✓ Duplicate pairs: {test_data['is_duplicate'].sum()}\")\n",
    "        print(f\"✓ Non-duplicate pairs: {len(test_data) - test_data['is_duplicate'].sum()}\")\n",
    "        \n",
    "        return test_data\n",
    "    \n",
    "    def predict_similarity(self, question_pairs, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Predict similarity for question pairs using SBERT + Cosine Similarity\n",
    "        \n",
    "        Args:\n",
    "            question_pairs: DataFrame with 'question1' and 'question2' columns\n",
    "            threshold: Similarity threshold for classification\n",
    "            \n",
    "        Returns:\n",
    "            predictions, similarity_scores\n",
    "        \"\"\"\n",
    "        print(f\"\\n=== GENERATING PREDICTIONS ===\")\n",
    "        print(f\"Processing {len(question_pairs)} question pairs...\")\n",
    "        \n",
    "        similarities = []\n",
    "        predictions = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Process in batches for efficiency\n",
    "        batch_size = 100\n",
    "        for i in range(0, len(question_pairs), batch_size):\n",
    "            batch = question_pairs.iloc[i:i+batch_size]\n",
    "            \n",
    "            # Get embeddings for both questions in batch\n",
    "            q1_embeddings = self.model.encode(batch['question1'].tolist())\n",
    "            q2_embeddings = self.model.encode(batch['question2'].tolist())\n",
    "            \n",
    "            # Calculate cosine similarities\n",
    "            for j in range(len(batch)):\n",
    "                similarity = cosine_similarity([q1_embeddings[j]], [q2_embeddings[j]])[0][0]\n",
    "                similarities.append(similarity)\n",
    "                predictions.append(1 if similarity >= threshold else 0)\n",
    "            \n",
    "            if i % 500 == 0:\n",
    "                print(f\"Processed {i + len(batch)} pairs...\")\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"✓ Predictions completed in {end_time - start_time:.2f} seconds\")\n",
    "        \n",
    "        return np.array(predictions), np.array(similarities)\n",
    "    \n",
    "    def evaluate_performance(self, y_true, y_pred, similarities, threshold=0.5):\n",
    "        \"\"\"Comprehensive performance evaluation\"\"\"\n",
    "        print(f\"\\n=== PERFORMANCE EVALUATION ===\")\n",
    "        print(f\"Similarity threshold: {threshold}\")\n",
    "        \n",
    "        # Basic metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "        \n",
    "        # AUC score\n",
    "        auc_score = roc_auc_score(y_true, similarities)\n",
    "        \n",
    "        results = {\n",
    "            'threshold': threshold,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'auc_score': auc_score,\n",
    "            'total_samples': len(y_true),\n",
    "            'true_positives': np.sum((y_true == 1) & (y_pred == 1)),\n",
    "            'true_negatives': np.sum((y_true == 0) & (y_pred == 0)),\n",
    "            'false_positives': np.sum((y_true == 0) & (y_pred == 1)),\n",
    "            'false_negatives': np.sum((y_true == 1) & (y_pred == 0))\n",
    "        }\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nPERFORMANCE METRICS:\")\n",
    "        print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "        print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%)\")\n",
    "        print(f\"F1-Score:  {f1:.4f} ({f1*100:.2f}%)\")\n",
    "        print(f\"AUC Score: {auc_score:.4f} ({auc_score*100:.2f}%)\")\n",
    "        \n",
    "        print(f\"\\nCONFUSION MATRIX BREAKDOWN:\")\n",
    "        print(f\"True Positives:  {results['true_positives']:4d}\")\n",
    "        print(f\"True Negatives:  {results['true_negatives']:4d}\")\n",
    "        print(f\"False Positives: {results['false_positives']:4d}\")\n",
    "        print(f\"False Negatives: {results['false_negatives']:4d}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def find_optimal_threshold(self, y_true, similarities):\n",
    "        \"\"\"Find the best threshold for classification\"\"\"\n",
    "        print(f\"\\n=== FINDING OPTIMAL THRESHOLD ===\")\n",
    "        \n",
    "        thresholds = np.arange(0.1, 1.0, 0.05)\n",
    "        best_f1 = 0\n",
    "        best_threshold = 0.5\n",
    "        threshold_results = []\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            y_pred = (similarities >= threshold).astype(int)\n",
    "            f1 = f1_score(y_true, y_pred)\n",
    "            \n",
    "            threshold_results.append({\n",
    "                'threshold': threshold,\n",
    "                'f1_score': f1,\n",
    "                'accuracy': accuracy_score(y_true, y_pred),\n",
    "                'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "                'recall': recall_score(y_true, y_pred, zero_division=0)\n",
    "            })\n",
    "            \n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_threshold = threshold\n",
    "        \n",
    "        print(f\"✓ Optimal threshold: {best_threshold:.3f}\")\n",
    "        print(f\"✓ Best F1-score: {best_f1:.4f}\")\n",
    "        \n",
    "        return best_threshold, threshold_results\n",
    "    \n",
    "    def analyze_examples(self, test_data, similarities, predictions, n_examples=5):\n",
    "        \"\"\"Analyze specific examples of predictions\"\"\"\n",
    "        print(f\"\\n=== EXAMPLE ANALYSIS ===\")\n",
    "        \n",
    "        # Add predictions to test data\n",
    "        test_data_copy = test_data.copy()\n",
    "        test_data_copy['similarity_score'] = similarities\n",
    "        test_data_copy['predicted'] = predictions\n",
    "        test_data_copy['correct'] = (test_data_copy['is_duplicate'] == test_data_copy['predicted'])\n",
    "        \n",
    "        print(f\"\\nCORRECT PREDICTIONS (High Confidence):\")\n",
    "        correct_high_conf = test_data_copy[\n",
    "            test_data_copy['correct'] & \n",
    "            ((test_data_copy['similarity_score'] > 0.8) | (test_data_copy['similarity_score'] < 0.2))\n",
    "        ].head(n_examples)\n",
    "        \n",
    "        for idx, row in correct_high_conf.iterrows():\n",
    "            print(f\"\\nSimilarity: {row['similarity_score']:.4f} | Predicted: {row['predicted']} | Actual: {row['is_duplicate']}\")\n",
    "            print(f\"Q1: {row['question1']}\")\n",
    "            print(f\"Q2: {row['question2']}\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        print(f\"\\n INCORRECT PREDICTIONS:\")\n",
    "        incorrect = test_data_copy[~test_data_copy['correct']].head(n_examples)\n",
    "        \n",
    "        for idx, row in incorrect.iterrows():\n",
    "            print(f\"\\nSimilarity: {row['similarity_score']:.4f} | Predicted: {row['predicted']} | Actual: {row['is_duplicate']}\")\n",
    "            print(f\"Q1: {row['question1']}\")\n",
    "            print(f\"Q2: {row['question2']}\")\n",
    "            print(\"-\" * 60)\n",
    "        \n",
    "        return test_data_copy\n",
    "    \n",
    "    def create_visualizations(self, threshold_results, similarities, y_true):\n",
    "        \"\"\"Create performance visualizations\"\"\"\n",
    "        print(f\"\\n=== CREATING VISUALIZATIONS ===\")\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Threshold vs Metrics\n",
    "        df_thresh = pd.DataFrame(threshold_results)\n",
    "        \n",
    "        axes[0,0].plot(df_thresh['threshold'], df_thresh['f1_score'], 'b-', label='F1-Score', linewidth=2)\n",
    "        axes[0,0].plot(df_thresh['threshold'], df_thresh['accuracy'], 'r-', label='Accuracy', linewidth=2)\n",
    "        axes[0,0].plot(df_thresh['threshold'], df_thresh['precision'], 'g-', label='Precision', linewidth=2)\n",
    "        axes[0,0].plot(df_thresh['threshold'], df_thresh['recall'], 'm-', label='Recall', linewidth=2)\n",
    "        axes[0,0].set_xlabel('Threshold')\n",
    "        axes[0,0].set_ylabel('Score')\n",
    "        axes[0,0].set_title('Performance Metrics vs Threshold')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Similarity Score Distribution\n",
    "        axes[0,1].hist(similarities[y_true == 0], bins=50, alpha=0.7, label='Non-Duplicates', color='red')\n",
    "        axes[0,1].hist(similarities[y_true == 1], bins=50, alpha=0.7, label='Duplicates', color='blue')\n",
    "        axes[0,1].set_xlabel('Cosine Similarity Score')\n",
    "        axes[0,1].set_ylabel('Frequency')\n",
    "        axes[0,1].set_title('Distribution of Similarity Scores')\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # 3. ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(y_true, similarities)\n",
    "        auc_score = roc_auc_score(y_true, similarities)\n",
    "        \n",
    "        axes[1,0].plot(fpr, tpr, color='blue', linewidth=2, label=f'ROC Curve (AUC = {auc_score:.4f})')\n",
    "        axes[1,0].plot([0, 1], [0, 1], color='red', linestyle='--', linewidth=1, label='Random')\n",
    "        axes[1,0].set_xlabel('False Positive Rate')\n",
    "        axes[1,0].set_ylabel('True Positive Rate')\n",
    "        axes[1,0].set_title('ROC Curve')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Confusion Matrix\n",
    "        best_threshold = df_thresh.loc[df_thresh['f1_score'].idxmax(), 'threshold']\n",
    "        y_pred_best = (similarities >= best_threshold).astype(int)\n",
    "        cm = confusion_matrix(y_true, y_pred_best)\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,1])\n",
    "        axes[1,1].set_xlabel('Predicted')\n",
    "        axes[1,1].set_ylabel('Actual')\n",
    "        axes[1,1].set_title(f'Confusion Matrix (Threshold: {best_threshold:.3f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"✓ Visualizations saved as 'evaluation_results.png'\")\n",
    "\n",
    "def run_complete_evaluation():\n",
    "    \"\"\"Run the complete evaluation pipeline\"\"\"\n",
    "    print(\"STARTING COMPLETE EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    evaluator = QuestionSimilarityEvaluator()\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = evaluator.load_test_data(sample_size=1000)  # Adjust sample size as needed\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_true = test_data['is_duplicate'].values\n",
    "    y_pred, similarities = evaluator.predict_similarity(test_data)\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    best_threshold, threshold_results = evaluator.find_optimal_threshold(y_true, similarities)\n",
    "    \n",
    "    # Evaluate with optimal threshold\n",
    "    y_pred_optimal = (similarities >= best_threshold).astype(int)\n",
    "    results = evaluator.evaluate_performance(y_true, y_pred_optimal, similarities, best_threshold)\n",
    "    \n",
    "    # Analyze examples\n",
    "    analysis_data = evaluator.analyze_examples(test_data, similarities, y_pred_optimal)\n",
    "    \n",
    "    # Create visualizations\n",
    "    evaluator.create_visualizations(threshold_results, similarities, y_true)\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df = pd.DataFrame(threshold_results)\n",
    "    results_df.to_csv('threshold_analysis.csv', index=False)\n",
    "    \n",
    "    analysis_data.to_csv('prediction_analysis.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nEVALUATION COMPLETE!\")\n",
    "    print(f\"✓ Results saved to CSV files\")\n",
    "    print(f\"✓ Visualizations saved as PNG\")\n",
    "    \n",
    "    return results, evaluator\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results, evaluator = run_complete_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
